{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uproot\n",
    "import numpy as np\n",
    "\n",
    "def load_root_file(file_path, branches=None, print_branches=False):\n",
    "    all_branches = {}\n",
    "    with uproot.open(file_path) as file:\n",
    "        tree = file[\"tree\"]\n",
    "        # Load all ROOT branches into array if not specified\n",
    "        if branches is None:\n",
    "            branches = tree.keys()\n",
    "        # Option to print the branch names\n",
    "        if print_branches:\n",
    "            print(\"Branches:\", tree.keys())\n",
    "        # Each branch is added to the dictionary\n",
    "        for branch in branches:\n",
    "            try:\n",
    "                all_branches[branch] = (tree[branch].array(library=\"np\"))\n",
    "            except uproot.KeyInFileError as e:\n",
    "                print(f\"KeyInFileError: {e}\")\n",
    "        # Number of events in file\n",
    "        all_branches['event'] = tree.num_entries\n",
    "    return all_branches\n",
    "\n",
    "branches_list = [\n",
    "    # Core pT3 properties\n",
    "    'tc_isDuplicate',\n",
    "    'tc_pt',\n",
    "    'tc_eta',\n",
    "    'tc_phi',\n",
    "    'tc_type',\n",
    "    'tc_matched_simIdx',\n",
    "]\n",
    "\n",
    "# Hit-dependent branches\n",
    "# suffixes = ['r', 'z', 'eta', 'phi', 'layer']\n",
    "# branches_list += [f't3_hit_{i}_{suffix}' for i in [0, 1, 2, 3, 4, 5] for suffix in suffixes]\n",
    "\n",
    "file_path = \"pt3_500_new.root\"\n",
    "branches = load_root_file(file_path, branches_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing basic branch info:\")\n",
    "print(len(branches['tc_matched_simIdx']))\n",
    "print(branches['tc_matched_simIdx'][0])\n",
    "print(branches['tc_matched_simIdx'][1])\n",
    "print(branches['tc_matched_simIdx'][2])\n",
    "print(len(branches['tc_matched_simIdx'][0]))\n",
    "print(len(branches['tc_matched_simIdx'][1]))\n",
    "print(len(branches['tc_matched_simIdx'][2]))\n",
    "print(branches[\"tc_pt\"][0])\n",
    "print(len(branches[\"tc_pt\"][0]))\n",
    "print(len(branches[\"tc_pt\"][1]))\n",
    "print(len(branches[\"tc_pt\"][2]))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Checking how many sim hits are matched to tracks:\")\n",
    "num_matched = {}\n",
    "for ev in range(len(branches['tc_matched_simIdx'])):\n",
    "    # print(f\"On event {ev}\")\n",
    "    for tk in range(len(branches['tc_matched_simIdx'][ev])):\n",
    "        n_matched = len(branches['tc_matched_simIdx'][ev][tk])\n",
    "        num_matched[n_matched] = num_matched.get(n_matched, 0) + 1\n",
    "print(num_matched)\n",
    "\n",
    "if False:\n",
    "    print(\"\")\n",
    "    print(\"Looking at results of extract_sim_indices:\")\n",
    "    xxxx = extract_sim_indices(branches['tc_matched_simIdx'])\n",
    "    print(type(xxxx))\n",
    "    print(len(xxxx))\n",
    "    print(len(xxxx[0]))\n",
    "    print(len(xxxx[1]))\n",
    "    print(len(xxxx[2]))\n",
    "    print(xxxx[0][:5])\n",
    "    print(xxxx[0][-5:])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uproot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Data Preprocessing Helpers\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def extract_sim_indices(sim_idx_branch):\n",
    "    \"\"\"\n",
    "    Extracts sim index integers from an array of STLVector objects.\n",
    "    Each event in sim_idx_branch is an STLVector of tracks,\n",
    "    and each track is itself an STLVector containing a single integer.\n",
    "    \"\"\"\n",
    "    extracted = []\n",
    "    for event in sim_idx_branch:\n",
    "        event_indices = []\n",
    "        for track in event:\n",
    "            if len(track) > 0:\n",
    "                event_indices.append(track[0])\n",
    "            else:\n",
    "                event_indices.append(None)\n",
    "        extracted.append(event_indices)\n",
    "    return extracted\n",
    "\n",
    "def create_pairs_balanced(features, sim_indices, max_duplicate_pairs_per_event=20, max_nonduplicate_pairs_per_event=90):\n",
    "    \"\"\"\n",
    "    For each event, generate duplicate pairs by grouping tracks with the same sim index.\n",
    "    Also, generate a set of non-duplicate pairs (tracks with different sim indices).\n",
    "    This routine returns a more balanced dataset.\n",
    "    \"\"\"\n",
    "    duplicate_left = []\n",
    "    duplicate_right = []\n",
    "    nonduplicate_left = []\n",
    "    nonduplicate_right = []\n",
    "    \n",
    "    for event_features, event_sim in zip(features, sim_indices):\n",
    "        n_tracks = event_features.shape[0]\n",
    "        # Build a dictionary mapping sim index to track indices.\n",
    "        sim_dict = {}\n",
    "        for idx, sim in enumerate(event_sim):\n",
    "            if sim is not None:\n",
    "                sim_dict.setdefault(sim, []).append(idx)\n",
    "        \n",
    "        # Duplicate pairs: for each sim index that appears more than once.\n",
    "        dup_pairs = []\n",
    "        for sim, indices in sim_dict.items():\n",
    "            if len(indices) > 1:\n",
    "                # Form all pairs among indices\n",
    "                for i in range(len(indices)):\n",
    "                    for j in range(i+1, len(indices)):\n",
    "                        dup_pairs.append((indices[i], indices[j]))\n",
    "        # Sample a maximum number per event if needed.\n",
    "        if len(dup_pairs) > max_duplicate_pairs_per_event:\n",
    "            dup_pairs = random.sample(dup_pairs, max_duplicate_pairs_per_event)\n",
    "        for i, j in dup_pairs:\n",
    "            duplicate_left.append(event_features[i])\n",
    "            duplicate_right.append(event_features[j])\n",
    "        \n",
    "        # Non-duplicate pairs: pairs with different sim indices.\n",
    "        nondup_pairs = []\n",
    "        for i in range(n_tracks):\n",
    "            for j in range(i+1, n_tracks):\n",
    "                # If either sim index is None or they differ, count as non-duplicate.\n",
    "                if event_sim[i] != event_sim[j]:\n",
    "                    nondup_pairs.append((i, j))\n",
    "        if len(nondup_pairs) > max_nonduplicate_pairs_per_event:\n",
    "            nondup_pairs = random.sample(nondup_pairs, max_nonduplicate_pairs_per_event)\n",
    "        for i, j in nondup_pairs:\n",
    "            nonduplicate_left.append(event_features[i])\n",
    "            nonduplicate_right.append(event_features[j])\n",
    "    \n",
    "    # Combine duplicate and non-duplicate pairs.\n",
    "    left = np.concatenate([np.array(duplicate_left), np.array(nonduplicate_left)], axis=0)\n",
    "    right = np.concatenate([np.array(duplicate_right), np.array(nonduplicate_right)], axis=0)\n",
    "    labels = np.concatenate([np.zeros(len(duplicate_left)), np.ones(len(nonduplicate_left))], axis=0)\n",
    "    return left, right, labels\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Prepare Data from Loaded Branches\n",
    "# ------------------------------------------------------------\n",
    "# Assume that your earlier code has loaded ROOT branches into the dictionary 'branches'.\n",
    "# We use these branches: tc_pt, tc_eta, tc_phi, tc_type, and tc_matched_simIdx.\n",
    "\n",
    "features = []\n",
    "n_events = len(branches['tc_pt'])\n",
    "for i in range(n_events):\n",
    "    # Create an array of shape (n_tracks, 4) for each event.\n",
    "    event_features = np.column_stack((\n",
    "        np.log10(branches['tc_pt'][i]),\n",
    "        (branches['tc_eta'][i]/4.0),\n",
    "        (branches['tc_phi'][i]/3.1415926),\n",
    "        (branches['tc_type'][i]-6.0)/2.0\n",
    "    ))\n",
    "    features.append(event_features)\n",
    "\n",
    "# Extract plain sim indices from the STLVector branch.\n",
    "sim_indices = extract_sim_indices(branches['tc_matched_simIdx'])\n",
    "\n",
    "# Generate balanced pairs from each event.\n",
    "X_left, X_right, y = create_pairs_balanced(features, sim_indices,\n",
    "                                             max_duplicate_pairs_per_event=30,\n",
    "                                             max_nonduplicate_pairs_per_event=270)\n",
    "\n",
    "print(\"Total pairs generated:\", len(y))\n",
    "print(\"Duplicate pairs (label 0):\", np.sum(y == 0))\n",
    "print(\"Non-duplicate pairs (label 1):\", np.sum(y == 1))\n",
    "\n",
    "# Split into training and testing sets.\n",
    "X_left_train, X_left_test, X_right_train, X_right_test, y_train, y_test = train_test_split(\n",
    "    X_left, X_right, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Create a PyTorch Dataset and DataLoader\n",
    "# ------------------------------------------------------------\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, X_left, X_right, y):\n",
    "        self.X_left = X_left.astype(np.float32)\n",
    "        self.X_right = X_right.astype(np.float32)\n",
    "        self.y = y.astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_left[idx], self.X_right[idx], self.y[idx]\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = SiameseDataset(X_left_train, X_right_train, y_train)\n",
    "test_dataset = SiameseDataset(X_left_test, X_right_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Define the Siamese Network in PyTorch\n",
    "# ------------------------------------------------------------\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_dim=4, embedding_dim=2):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 16)  # New hidden layer with 16 units\n",
    "        self.fc3 = nn.Linear(16, embedding_dim)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.embedding_net(x1)\n",
    "        out2 = self.embedding_net(x2)\n",
    "        # Compute Euclidean distance between the two embeddings.\n",
    "        distance = torch.sqrt(torch.sum((out1 - out2)**2, dim=1, keepdim=True) + 1e-6)\n",
    "        return distance\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, distance, label):\n",
    "        # For duplicate pairs (label 0): loss = distance^2.\n",
    "        # For non-duplicates (label 1): loss = max(margin - distance, 0)^2.\n",
    "        loss_similar = (1 - label) * torch.pow(distance, 2)\n",
    "        loss_dissimilar = label * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        loss = torch.mean(loss_similar + loss_dissimilar)\n",
    "        return loss\n",
    "\n",
    "input_dim = 4       # four input features\n",
    "embedding_dim = 4   # embedding size\n",
    "\n",
    "embedding_net = EmbeddingNet(input_dim, embedding_dim)\n",
    "model = SiameseNet(embedding_net)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Train the Model\n",
    "# ------------------------------------------------------------\n",
    "num_epochs = 300 # 300\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_left, batch_right, batch_label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        distances = model(batch_left, batch_right)\n",
    "        loss = criterion(distances, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_left.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Validation step.\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_left, batch_right, batch_label in test_loader:\n",
    "            distances = model(batch_left, batch_right)\n",
    "            loss = criterion(distances, batch_label)\n",
    "            running_val_loss += loss.item() * batch_left.size(0)\n",
    "    epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {epoch_val_loss:.4f} - now {time.strftime(\"%Y_%m_%d_%Hh%Mm%Ss\")}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Performance Evaluation and Plotting\n",
    "# ------------------------------------------------------------\n",
    "model.eval()\n",
    "all_distances = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_left, batch_right, batch_label in test_loader:\n",
    "        distances = model(batch_left, batch_right)\n",
    "        all_distances.append(distances.cpu().numpy())\n",
    "        all_labels.append(batch_label.cpu().numpy())\n",
    "\n",
    "all_distances = np.concatenate(all_distances).flatten()\n",
    "all_labels = np.concatenate(all_labels).flatten()\n",
    "\n",
    "print(\"Test distances range: \", all_distances.min(), all_distances.max())\n",
    "\n",
    "# For a range of thresholds, count duplicate pairs (true positives) and non-duplicate pairs (false positives)\n",
    "thresholds = np.linspace(all_distances.min(), all_distances.max(), 100)\n",
    "tp_list = []  # duplicate pairs correctly identified (true positives)\n",
    "fp_list = []  # non-duplicate pairs incorrectly flagged (false positives)\n",
    "\n",
    "for t in thresholds:\n",
    "    pred_duplicate = all_distances < t  # predict duplicate if distance < threshold\n",
    "    tp = np.sum((pred_duplicate == True) & (all_labels == 0))\n",
    "    fp = np.sum((pred_duplicate == True) & (all_labels == 1))\n",
    "    tp_list.append(tp)\n",
    "    fp_list.append(fp)\n",
    "\n",
    "# Plot the performance curve.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fp_list, tp_list, marker='o')\n",
    "plt.xlabel(\"Non-Duplicate Pairs (False Positives) Removed\")\n",
    "plt.ylabel(\"Duplicate Pairs (True Positives) Removed\")\n",
    "plt.title(\"Duplicate Removal Performance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss history.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Contrastive Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Performance Evaluation and Additional Plots\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "all_distances = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_left, batch_right, batch_label in test_loader:\n",
    "        distances = model(batch_left, batch_right)\n",
    "        all_distances.append(distances.cpu().numpy())\n",
    "        all_labels.append(batch_label.cpu().numpy())\n",
    "\n",
    "all_distances = np.concatenate(all_distances).flatten()\n",
    "all_labels = np.concatenate(all_labels).flatten()\n",
    "\n",
    "print(\"Test distances range: \", all_distances.min(), all_distances.max())\n",
    "\n",
    "# ----------------------------\n",
    "# Plot 1: Performance at 99.5% Non-Duplicate Retention\n",
    "# ----------------------------\n",
    "# Separate distances by label.\n",
    "nondup_distances = all_distances[all_labels==1]  # non-duplicate pairs\n",
    "dup_distances = all_distances[all_labels==0]      # duplicate pairs\n",
    "\n",
    "# Determine threshold such that 99.5% of non-duplicate pairs are retained (i.e. only 0.5% fall below the threshold).\n",
    "threshold_nondup = np.percentile(nondup_distances, 0.0001)\n",
    "print(\"Threshold for 99.99% retention of non-duplicates:\", threshold_nondup)\n",
    "\n",
    "# Count how many duplicate pairs would be rejected (i.e. have distance below the threshold).\n",
    "tp_at_threshold = np.sum(dup_distances < threshold_nondup)\n",
    "total_dup = len(dup_distances)\n",
    "print(\"Duplicate pairs rejected (distance < threshold):\", tp_at_threshold, \"out of\", total_dup)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(nondup_distances, bins=50, alpha=0.5, label=\"Non-Duplicate Pairs\")\n",
    "plt.hist(dup_distances, bins=50, alpha=0.5, label=\"Duplicate Pairs\")\n",
    "plt.axvline(x=threshold_nondup, color='red', linestyle='--', \n",
    "            label=f\"Threshold (99.99% non-dup kept)\\n{threshold_nondup:.3f}\")\n",
    "plt.xlabel(\"Euclidean Distance\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Pair Distances\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Plot 2: Embedding Space Visualization\n",
    "# ----------------------------\n",
    "# Choose one event (e.g., event 0) to visualize its tracks in the embedding space.\n",
    "event_idx = 0  # you can change this to any event you like\n",
    "event_features = torch.tensor(features[event_idx].astype(np.float32))\n",
    "with torch.no_grad():\n",
    "    event_embeddings = embedding_net(event_features).cpu().numpy()\n",
    "\n",
    "# Get the corresponding sim indices for this event.\n",
    "event_sim = sim_indices[event_idx]\n",
    "# For visualization, if sim index is None, assign a fixed value (-1).\n",
    "colors = np.array([sim if sim is not None else -1 for sim in event_sim], dtype=float)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sc = plt.scatter(event_embeddings[:, 0], event_embeddings[:, 1], c=colors, cmap='viridis', s=50)\n",
    "plt.xlabel(\"Embedding Dimension 1\")\n",
    "plt.ylabel(\"Embedding Dimension 2\")\n",
    "plt.title(f\"Embedding Space for Event {event_idx}\")\n",
    "plt.colorbar(sc, label=\"Sim Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# Choose an event index to visualize\n",
    "# ----------------------------\n",
    "event_idx = 0  # Change to the event you want to plot\n",
    "\n",
    "# Convert the features for this event to a PyTorch tensor.\n",
    "event_features = torch.tensor(features[event_idx].astype(np.float32))\n",
    "\n",
    "# Forward pass through the embedding network to get 4D embeddings (shape: [n_tracks, 4])\n",
    "with torch.no_grad():\n",
    "    event_embeddings = embedding_net(event_features).cpu().numpy()\n",
    "\n",
    "# Retrieve track types for this event (shape: [n_tracks])\n",
    "event_types = branches['tc_type'][event_idx]\n",
    "\n",
    "# We'll plot these dimension pairs from the 4D embedding space.\n",
    "dim_pairs = [(0, 1), (0, 2), (0, 3),\n",
    "             (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "# Create a figure with subplots for each pair\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axs = axs.flatten()  # Flatten so we can index easily\n",
    "\n",
    "# Define a dictionary that assigns each tc_type a distinct color.\n",
    "type_color_map = {\n",
    "    4: 'red',\n",
    "    5: 'blue',\n",
    "    7: 'green',\n",
    "    8: 'purple'\n",
    "}\n",
    "\n",
    "# Define a dictionary that maps each tc_type to a more descriptive label.\n",
    "type_label_map = {\n",
    "    4: 'pT5',\n",
    "    5: 'pT3',\n",
    "    7: 'T5',\n",
    "    8: 'pLS'\n",
    "}\n",
    "\n",
    "for idx, (x_dim, y_dim) in enumerate(dim_pairs):\n",
    "    ax = axs[idx]\n",
    "    \n",
    "    # Plot each point according to its tc_type\n",
    "    for i, ttype in enumerate(event_types):\n",
    "        # Look up the color and label. If unknown type, fallback to \"gray\" / \"unknown\".\n",
    "        color = type_color_map.get(ttype, 'gray')\n",
    "        label = type_label_map.get(ttype, 'unknown')\n",
    "        \n",
    "        ax.scatter(\n",
    "            event_embeddings[i, x_dim],\n",
    "            event_embeddings[i, y_dim],\n",
    "            color=color,\n",
    "            s=40,\n",
    "            alpha=0.7,\n",
    "            edgecolors='k',\n",
    "            label=label  # We'll handle duplicates in the legend below\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel(f\"Embedding Dim {x_dim}\")\n",
    "    ax.set_ylabel(f\"Embedding Dim {y_dim}\")\n",
    "    ax.set_title(f\"Dims {x_dim} vs {y_dim}\")\n",
    "\n",
    "    # Collect legend entries for this subplot, dropping duplicates\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = dict(zip(labels, handles))  # label -> handle\n",
    "    ax.legend(unique.values(), unique.keys(), loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# Choose an event index to visualize\n",
    "# ----------------------------\n",
    "event_idx = 0  # Change to the event you want to plot\n",
    "\n",
    "# Convert the features for this event to a PyTorch tensor and embed them.\n",
    "event_features = torch.tensor(features[event_idx].astype(np.float32))\n",
    "with torch.no_grad():\n",
    "    event_embeddings = embedding_net(event_features).cpu().numpy()  # shape: [n_tracks, 4]\n",
    "\n",
    "# Retrieve the duplicate flags for this event (0 or 1 per track)\n",
    "event_isdup = branches['tc_isDuplicate'][event_idx]  # shape: [n_tracks]\n",
    "\n",
    "# We'll plot these dimension pairs from the 4D embedding space.\n",
    "dim_pairs = [(0, 1), (0, 2), (0, 3),\n",
    "             (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "# Create a figure with subplots for each pair\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axs = axs.flatten()  # Flatten so we can index easily\n",
    "\n",
    "for idx, (x_dim, y_dim) in enumerate(dim_pairs):\n",
    "    ax = axs[idx]\n",
    "\n",
    "    # Create boolean masks for duplicates vs. non-duplicates\n",
    "    mask_dup = (event_isdup == 1)\n",
    "    mask_nondup = (event_isdup == 0)\n",
    "\n",
    "    # Plot non-duplicate tracks in blue\n",
    "    ax.scatter(\n",
    "        event_embeddings[mask_nondup, x_dim],\n",
    "        event_embeddings[mask_nondup, y_dim],\n",
    "        color='blue',\n",
    "        s=40,\n",
    "        alpha=0.7,\n",
    "        edgecolors='k',\n",
    "        label='Non-Duplicate Tracks'\n",
    "    )\n",
    "\n",
    "    # Plot duplicate tracks in red\n",
    "    ax.scatter(\n",
    "        event_embeddings[mask_dup, x_dim],\n",
    "        event_embeddings[mask_dup, y_dim],\n",
    "        color='red',\n",
    "        s=40,\n",
    "        alpha=0.7,\n",
    "        edgecolors='k',\n",
    "        label='Duplicate Tracks'\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel(f\"Embedding Dim {x_dim}\")\n",
    "    ax.set_ylabel(f\"Embedding Dim {y_dim}\")\n",
    "    ax.set_title(f\"Dims {x_dim} vs {y_dim}\")\n",
    "\n",
    "    # Add a legend to each subplot, removing duplicate entries\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # Convert to a dict to drop duplicates\n",
    "    unique = dict(zip(labels, handles))\n",
    "    ax.legend(unique.values(), unique.keys(), loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_events = 20\n",
    "\n",
    "# Initialize accumulators.\n",
    "sum_removed_nondup_real = 0\n",
    "sum_total_real = 0\n",
    "\n",
    "sum_removed_duplicates = 0\n",
    "sum_total_duplicates = 0\n",
    "\n",
    "for event_idx in range(num_events):\n",
    "    # Get features and compute embeddings for event_idx\n",
    "    event_features = torch.tensor(features[event_idx].astype(np.float32))\n",
    "    with torch.no_grad():\n",
    "        event_embeddings = embedding_net(event_features).cpu().numpy()\n",
    "    \n",
    "    # Get the duplicate flag and sim indices for each track in the event.\n",
    "    event_isdup = branches['tc_isDuplicate'][event_idx]  # Boolean or 0/1\n",
    "    event_sim = sim_indices[event_idx]  # None => fake track, else real track index\n",
    "    \n",
    "    n_tracks = event_embeddings.shape[0]\n",
    "    \n",
    "    # Count total real and fake tracks in this event.\n",
    "    total_real = sum(1 for s in event_sim if s is not None)\n",
    "    total_fake = n_tracks - total_real\n",
    "    \n",
    "    # Count how many are flagged as duplicates vs. non-duplicates before removal.\n",
    "    total_duplicates_before = sum(1 for flag in event_isdup if flag)\n",
    "    total_nonduplicates_before = n_tracks - total_duplicates_before\n",
    "    \n",
    "    print(f\"Event {event_idx}:\")\n",
    "    print(\"Before duplicate removal:\")\n",
    "    print(f\"  Total tracks: {n_tracks}\")\n",
    "    print(f\"  Real tracks: {total_real}\")\n",
    "    print(f\"  Fake tracks: {total_fake}\")\n",
    "    print(f\"  Duplicate tracks: {total_duplicates_before}\")\n",
    "    print(f\"  Non-duplicate tracks: {total_nonduplicates_before}\")\n",
    "    \n",
    "    # --- Duplicate Removal: Greedy Algorithm ---\n",
    "    threshold = 0.05\n",
    "    keep_mask = np.ones(n_tracks, dtype=bool)\n",
    "    \n",
    "    for i in range(n_tracks):\n",
    "        if not keep_mask[i]:\n",
    "            continue\n",
    "        for j in range(i + 1, n_tracks):\n",
    "            if not keep_mask[j]:\n",
    "                continue\n",
    "            d = np.linalg.norm(event_embeddings[i] - event_embeddings[j])\n",
    "            if d < threshold:\n",
    "                keep_mask[j] = False\n",
    "                \n",
    "    after_count = np.sum(keep_mask)\n",
    "    removed_count = n_tracks - after_count\n",
    "    \n",
    "    # Post-removal statistics\n",
    "    removed_indices = np.where(~keep_mask)[0]\n",
    "    removed_duplicates = sum(1 for i in removed_indices if event_isdup[i])\n",
    "    removed_nonduplicates = removed_count - removed_duplicates\n",
    "    \n",
    "    # Among removed non-duplicates, count real vs. fake.\n",
    "    removed_nondup_real = sum(\n",
    "        1 for i in removed_indices if (not event_isdup[i] and event_sim[i] is not None)\n",
    "    )\n",
    "    removed_nondup_fake = sum(\n",
    "        1 for i in removed_indices if (not event_isdup[i] and event_sim[i] is None)\n",
    "    )\n",
    "    \n",
    "    print(\"After duplicate removal:\")\n",
    "    print(f\"  Total tracks kept: {after_count}\")\n",
    "    print(f\"  Total tracks removed: {removed_count}\")\n",
    "    print(f\"  Removed tracks flagged as duplicates: {removed_duplicates}\")\n",
    "    print(f\"  Removed tracks flagged as non-duplicates: {removed_nonduplicates}\")\n",
    "    print(f\"     - Of these non-duplicates, real tracks removed: {removed_nondup_real}\")\n",
    "    print(f\"     - Of these non-duplicates, fake tracks removed: {removed_nondup_fake}\")\n",
    "    print(\"----------\")\n",
    "    \n",
    "    # Accumulate for overall statistics\n",
    "    sum_removed_nondup_real += removed_nondup_real\n",
    "    sum_total_real += total_real\n",
    "    \n",
    "    sum_removed_duplicates += removed_duplicates\n",
    "    sum_total_duplicates += total_duplicates_before\n",
    "\n",
    "# ---- After processing all events, compute overall percentages. ----\n",
    "\n",
    "if sum_total_real > 0:\n",
    "    percent_real_removed = 100.0 * sum_removed_nondup_real / sum_total_real\n",
    "else:\n",
    "    percent_real_removed = 0.0\n",
    "\n",
    "if sum_total_duplicates > 0:\n",
    "    percent_duplicates_removed = 100.0 * sum_removed_duplicates / sum_total_duplicates\n",
    "else:\n",
    "    percent_duplicates_removed = 0.0\n",
    "\n",
    "print(\"=== Overall Summary (across all events) ===\")\n",
    "print(f\"Percent of real tracks removed (non-duplicate real tracks): {percent_real_removed:.2f}%\")\n",
    "print(f\"Percent of duplicates removed: {percent_duplicates_removed:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_weights_biases(weights, biases, layer_name):\n",
    "    # Print biases\n",
    "    print(f\"HOST_DEVICE_CONSTANT float bias_{layer_name}[{len(biases)}] = {{\")\n",
    "    print(\", \".join(f\"{b:.7f}f\" for b in biases) + \" };\")\n",
    "    print()\n",
    "\n",
    "    # Print weights\n",
    "    print(f\"HOST_DEVICE_CONSTANT float wgtT_{layer_name}[{len(weights[0])}][{len(weights)}] = {{\")\n",
    "    for row in weights.T:\n",
    "        formatted_row = \", \".join(f\"{w:.7f}f\" for w in row)\n",
    "        print(f\"{{ {formatted_row} }},\")\n",
    "    print(\"};\")\n",
    "    print()\n",
    "\n",
    "def print_model_weights_biases(model):\n",
    "    # Make sure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through all named modules in the model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is a linear layer\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Get weights and biases\n",
    "            weights = module.weight.data.cpu().numpy()\n",
    "            biases = module.bias.data.cpu().numpy()\n",
    "\n",
    "            # Print formatted weights and biases\n",
    "            print_formatted_weights_biases(weights, biases, name.replace('.', '_'))\n",
    "\n",
    "print_model_weights_biases(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break\n",
    "# Avoiding a giant ass line break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing input features with embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference: input scaling\n",
    "#   np.log10(branches['tc_pt'][i]),\n",
    "#   (branches['tc_eta'][i]/4.0),\n",
    "#   (branches['tc_phi'][i]/3.1415926),\n",
    "#   (branches['tc_type'][i]-6.0)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "def plot():\n",
    "    n_inp = 4\n",
    "    n_emb = 4\n",
    "    \n",
    "    # event_idx = 0\n",
    "    # event_features = torch.tensor(features[event_idx].astype(np.float32))\n",
    "    print(\"len(features)\", len(features))\n",
    "    event_idxs = slice(0, 175)\n",
    "    event_features = torch.tensor(np.concat(features[event_idxs]).astype(np.float32))\n",
    "    with torch.no_grad():\n",
    "        event_embeddings = embedding_net(event_features).cpu().numpy()\n",
    "    print(event_features.shape)\n",
    "    print(event_embeddings.shape)\n",
    "\n",
    "    titles_inp = [\n",
    "        \"log10(pT)\",\n",
    "        \"eta (scaled)\",\n",
    "        \"phi (scaled)\",\n",
    "        \"type (scaled)\",\n",
    "    ]\n",
    "    bins_inp = [\n",
    "        np.arange(-0.1, 2.1, 0.05),\n",
    "        np.arange(-1.1, 1.2, 0.05),\n",
    "        np.arange(-1.1, 1.2, 0.05),\n",
    "        np.arange(-1.1, 1.2, 0.2),\n",
    "    ]\n",
    "    bins_emb = [\n",
    "        np.arange(-5, 15, 0.1),\n",
    "        np.arange(-2, 5, 0.1),\n",
    "        np.arange(-11, 6, 0.1),\n",
    "        np.arange(-1, 3, 0.05),\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2 + n_emb, ncols=n_inp, figsize=(14, 24))\n",
    "\n",
    "    for idx in range(n_inp):\n",
    "        ax[0, idx].hist(event_features[:, idx], bins=bins_inp[idx])\n",
    "        ax[0, idx].set_xlabel(titles_inp[idx])\n",
    "\n",
    "    for idx in range(n_emb):\n",
    "        ax[1, idx].hist(event_embeddings[:, idx], bins=bins_emb[idx])\n",
    "        ax[1, idx].set_xlabel(f\"Embedding dimension {idx}\")\n",
    "\n",
    "    for inp in range(n_inp):\n",
    "        #if inp != 0:\n",
    "        #    continue\n",
    "        for emb in range(n_emb):\n",
    "            _, _, _, im = ax[2+inp, emb].hist2d(event_features[:, inp],\n",
    "                                            event_embeddings[:, emb],\n",
    "                                            bins=[bins_inp[inp],\n",
    "                                                  bins_emb[emb]],\n",
    "                                            cmin=0.5,\n",
    "                                           )\n",
    "            ax[2+inp, emb].set_xlabel(titles_inp[inp])\n",
    "            ax[2+inp, emb].set_ylabel(f\"Embedding dimension {emb}\")\n",
    "            \n",
    "\n",
    "plot()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing humanoid quantities with embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference: input scaling\n",
    "#   np.log10(branches['tc_pt'][i]),\n",
    "#   (branches['tc_eta'][i]/4.0),\n",
    "#   (branches['tc_phi'][i]/3.1415926),\n",
    "#   (branches['tc_type'][i]-6.0)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dangle(x, y):\n",
    "    return np.min([(2 * np.pi) - np.abs(x - y), np.abs(x - y)], axis=0)\n",
    "\n",
    "def plot():\n",
    "    distances, labels, dpts, detas, dphis, drs = [], [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_left, batch_right, batch_label in test_loader:\n",
    "            ds = model(batch_left, batch_right)\n",
    "            pt_l = (10 ** batch_left[:, 0]).cpu().numpy()\n",
    "            pt_r = (10 ** batch_right[:, 0]).cpu().numpy()\n",
    "            eta_l = (4 * batch_left[:, 1]).cpu().numpy()\n",
    "            eta_r = (4 * batch_right[:, 1]).cpu().numpy()\n",
    "            phi_l = (3.1415926 * batch_left[:, 2]).cpu().numpy()\n",
    "            phi_r = (3.1415926 * batch_right[:, 2]).cpu().numpy()\n",
    "            # print(batch_left.shape)\n",
    "            dpts.append(pt_l - pt_r)\n",
    "            detas.append(eta_l - eta_r)\n",
    "            dphis.append(dangle(phi_l, phi_r))\n",
    "            drs.append( ((eta_l - eta_r)**2 + (dangle(phi_l, phi_r))**2) ** 0.5 )\n",
    "            labels.append(batch_label.cpu().numpy())\n",
    "            distances.append(ds)\n",
    "            # break\n",
    "\n",
    "    def flatten(li):\n",
    "        return np.concatenate(li).flatten()\n",
    "\n",
    "    distances = flatten(distances)\n",
    "    labels = np.abs(flatten(labels))\n",
    "    dpts = np.abs(flatten(dpts))\n",
    "    detas = np.abs(flatten(detas))\n",
    "    dphis = np.abs(flatten(dphis))\n",
    "    drs = np.abs(flatten(drs))\n",
    "    print(distances.shape)\n",
    "    print(labels.shape)\n",
    "    print(dpts.shape)\n",
    "    print(detas.shape)\n",
    "    print(dphis.shape)\n",
    "    print(drs.shape)\n",
    "\n",
    "    masks = [\n",
    "        labels == 0,\n",
    "        labels == 1,\n",
    "    ]\n",
    "    bins_distance = np.arange(0, 16, 0.1)\n",
    "    bins_dpt = np.arange(-1, 5, 0.1)\n",
    "    bins_deta = np.arange(0, 7, 0.1)\n",
    "    bins_dphi = np.arange(0, 3.25, 0.05)\n",
    "    bins_dr = np.arange(0, 7, 0.1)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(14, 12))\n",
    "\n",
    "    for idx, mask in enumerate(masks):\n",
    "        ax[idx, 0].hist(dpts[mask], bins=bins_dpt)\n",
    "        ax[idx, 1].hist(detas[mask], bins=bins_deta)\n",
    "        ax[idx, 2].hist(dphis[mask], bins=bins_dphi)\n",
    "        ax[idx, 3].hist(drs[mask], bins=bins_dr)\n",
    "\n",
    "        ax[idx+2, 0].hist2d(dpts[mask], distances[mask], cmin=0.5, bins=[bins_dpt, bins_distance])\n",
    "        ax[idx+2, 1].hist2d(detas[mask], distances[mask], cmin=0.5, bins=[bins_deta, bins_distance])\n",
    "        ax[idx+2, 2].hist2d(dphis[mask], distances[mask], cmin=0.5, bins=[bins_dphi, bins_distance])\n",
    "        ax[idx+2, 3].hist2d(drs[mask], distances[mask], cmin=0.5, bins=[bins_dr, bins_distance])\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
